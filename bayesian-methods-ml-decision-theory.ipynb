{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision theory and Bayesian methods: loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "- C. Davidson-Pilon, \"Bayesian methods for hackers\", Addison-Wesley (2016). A version updated to PyMC3 is available [here](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) and this notebook follows part of Chapter 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "from scipy.optimize import fmin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function that is asymmetric in the sign of the error between the true return and the predicted one: we want to penalise (big loss) the cases in which we are predicting a return that has a different sign from the true one. This will result in more conservative choices when we are predicting a gain when we'd in fact have a loss and when we are predicting a loss when we'd in fact have a gain.\n",
    "\n",
    "The function is\n",
    "$$\n",
    "L(R, \\hat{y}) \\equiv \\left\\lbrace\n",
    "\\begin{array}{l}\n",
    "\\alpha\\hat{y}^2 - \\text{sgn}(R)\\,\\hat{y} + |R|,\\, \\text{if}\\, R \\hat{y}<0\\\\\n",
    "|R - \\hat{y}|,\\, \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $R$ is the true return and $\\hat{y}$ is the predicted one and $\\alpha$ is an arbitrary (big) coefficient. This function grows linearly with the error when the prediction and the true return have the same sign, but in the opposite case grows quadratically _with the prediction_ (but not necessarily with the error $(R - \\hat{y})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_loss(true_return, prediction, alpha=100.):\n",
    "    \"\"\"\n",
    "    Loss function, asymmetric between the case in which prediction\n",
    "    and true value have the same sign and the one in which the sign\n",
    "    is opposite.\n",
    "    \"\"\"\n",
    "    if true_return*prediction < 0:\n",
    "        return alpha*prediction**2 -np.sign(true_return)*prediction + np.abs(true_return)\n",
    "    else:\n",
    "        return np.abs(true_return - prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_pos = go.Scatter(\n",
    "    x=np.linspace(-0.1, 0.1, 100),\n",
    "    y=[stock_loss(0.05, pred) for pred in np.linspace(-0.1, 0.1, 100)],\n",
    "    name='True value 0.05'\n",
    ")\n",
    "\n",
    "trace_neg = go.Scatter(\n",
    "    x=np.linspace(-0.1, 0.1, 100),\n",
    "    y=[stock_loss(-0.02, pred) for pred in np.linspace(-0.1, 0.1, 100)],\n",
    "    name='True value -0.02'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title='prediction'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='loss'\n",
    "    ),\n",
    "    shapes=[{\n",
    "            'type': 'line',\n",
    "            'x0': 0,\n",
    "            'y0': 0,\n",
    "            'x1': 0,\n",
    "            'y1': 1.5\n",
    "        }]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_pos, trace_neg], layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate artificial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate artificial data that is normally distributed around a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "true_params = {\n",
    "    'true_slope': 0.5,\n",
    "    'true_intercept': 0.0\n",
    "}\n",
    "\n",
    "X = np.random.uniform(-0.06, 0.06, N)\n",
    "Y = true_params['true_slope']*X + true_params['true_intercept'] + 0.01*np.random.randn(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_data = go.Scatter(\n",
    "    x=X,\n",
    "    y=Y,\n",
    "    mode='markers',\n",
    "    name='data'\n",
    ")\n",
    "\n",
    "trace_true = go.Scatter(\n",
    "    x=X,\n",
    "    y=true_params['true_slope']*X + true_params['true_intercept'],\n",
    "    name='true curve'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title='signal'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='return'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_data, trace_true], layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a model to the data with the form\n",
    "$$\n",
    "R = a + bx + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $a$ and $b$ are the unknown parameters and $\\epsilon \\sim \\text{Normal}(0, \\sigma)$ is normally-distributed random noise with unknown parameter $\\sigma$.\n",
    "\n",
    "We will use a Markov Chain Monte Carlo method to generate samples from the posterior distributions of the parameters $a, b, \\sigma$, specifying the following priors for them\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "a \\sim \\text{Normal}(0, 100),\\\\\n",
    "b \\sim \\text{Normal}(0, 100),\\\\\n",
    "\\sigma \\sim \\text{Uniform}(0, 100)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The model can be rewritten as a normal distribution centered on $a + bx$ with standard deviation $\\sigma$, so we have\n",
    "\n",
    "$$\n",
    "R \\sim \\text{Normal}(\\mu, \\sigma),\n",
    "$$\n",
    "\n",
    "with $\\mu = a + bx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    a = pm.Normal(\"a\", mu=0, sd=100.)\n",
    "    b = pm.Normal(\"b\", mu=0, sd=100.)\n",
    "    \n",
    "    sigma = pm.Uniform(\"sigma\", 0, 100)\n",
    "    \n",
    "    mu = pm.Deterministic(\"mu\", a + b*X)\n",
    "    \n",
    "    obs = pm.Normal(\"obs\", mu=mu, sd=sigma, observed=Y)\n",
    "    \n",
    "    trace = pm.sample(100000, step=pm.Metropolis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discard the first 20000 samples from the posterior distributions, as the Markov Chain might not have converged yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_trace = trace[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.plots.traceplot(trace=burned_trace, varnames=[\"a\", \"b\", \"sigma\"])\n",
    "pm.plot_posterior(trace=burned_trace, varnames=[\"a\", \"b\", \"sigma\"], kde_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any particular value $x$ of the signal we can now compute a full distribution for the return given by our model evaluated with all the different values drawn from the posterior distributions or the parameters it depends upon.\n",
    "\n",
    "Each element of the trace the MCMC returned is of the form $(a_i, b_i, \\sigma_i)$, where the index $i$ runs over all the sampled values. Given the value $x$ then we have the whole population of returns given by\n",
    "$$\n",
    "R_i = a_i + b_i x + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a sample from the distribution $\\text{Normal}(0, \\sigma_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the optimal returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a value $x$ for the signal, we can make a prediction $r$ for the return, and given this we can compute the expectation value of the loss function over the distribution of \"true returns\" associated with $x$,\n",
    "$$\n",
    "\\mathbb{E}_{R(x)} \\Bigl[ L(R(x), r) \\Bigr].\n",
    "$$\n",
    "\n",
    "From a Bayesian point of view the optimal prediction is the one that minimises the above expectation value,\n",
    "$$\n",
    "r_\\text{opt} = \\text{argmin}_r\\,\\mathbb{E}_{R(x)} \\Bigl[ L(R(x), r) \\Bigr].\n",
    "$$\n",
    "\n",
    "The minimum of this function can be computed numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorised_stock_loss(true_return, prediction, alpha=500.):\n",
    "    \"\"\"\n",
    "    Loss function, vectorised.\n",
    "    \"\"\"\n",
    "    loss = np.zeros_like(true_return)\n",
    "    ix = true_return*prediction < 0\n",
    "    loss[ix] = alpha*prediction**2 - np.sign(true_return[ix])*prediction + np.abs(true_return[ix])\n",
    "    loss[~ix] = np.abs(true_return[~ix] - prediction)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = burned_trace['a']\n",
    "b_samples = burned_trace['b']\n",
    "sigma_trace = burned_trace['sigma']\n",
    "\n",
    "num_samples = a_samples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to sample the distribution for the random noise.\n",
    "noise_generator = pm.Normal.dist(mu=0., sd=sigma_trace)\n",
    "\n",
    "noise = noise_generator.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_population = lambda signal: a_samples + b_samples*signal + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the optimal prediction for 50 values of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals = np.linspace(X.min(), X.max(), 50)\n",
    "\n",
    "opt_predictions = np.zeros_like(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, signal in tqdm(enumerate(signals)):\n",
    "    exp_value = lambda pred: vectorised_stock_loss(returns_population(signal), pred).mean()\n",
    "    \n",
    "    opt_predictions[i] = fmin(exp_value, 0, disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the maximum likelihood predictions (a simple linear regression) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X.reshape(-1,1), Y)\n",
    "\n",
    "mle_pred = lr.predict(signals.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Bayesian optimal prediction with the maximum likelihood one: as a first check we can observe that for very clear (positive or negative signals) the Bayesian optimal prediction converges to the MLE one. The new feature though is that when the signal is not that certain, meaning that the outcome could be positive or negative, the Bayesian optimal prediction gets closer to 0 than the MLE one, essentially suggesting not to take a position on the outcome: the loss function is telling us that the signal is too uncertain to make a reliable prediction, thus the most optimal strategy to minimise (the expectation value of) loss, is to predict 0 and wait for clearer signals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_bayes_pred = go.Scatter(\n",
    "    x=signals,\n",
    "    y=opt_predictions,\n",
    "    name='Bayesian optimal predictions'\n",
    ")\n",
    "\n",
    "trace_mle_pred = go.Scatter(\n",
    "    x=signals,\n",
    "    y=mle_pred,\n",
    "    name='MLE predictions'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title='signal'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='prediction'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_bayes_pred, trace_mle_pred], layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
